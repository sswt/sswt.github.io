# 5-Day Gen AI Intensive Course With Google

Here are the contents of the emails from kaggle to those who registered for the course.

Source [link](https://rsvp.withgoogle.com/events/google-generative-ai-intensive), [YT playlist](https://www.youtube.com/playlist?list=PLqFaTIg4myu-b1PlxitQdY0UYIbys-2es)
## Day 1 Foundation models and prompt engineering

**Assignments**

1. Complete the Intro Unit – “Foundational Large Language Models & Text Generation”, which is:

	- [Optional] Listen to the summary [podcast episode](https://www.youtube.com/watch?v=mQDlCZZsOyo&feature=youtu.be) (31 min) for this unit (created by [NotebookLM](https://notebooklm.google/)).
	-  Read the [“Foundational Large Language Models & Text Generation” whitepaper](https://www.kaggle.com/whitepaper-foundational-llm-and-text-generation) (75 pages).

2. Complete Unit 1 – “Prompt Engineering”, which is:

	-  [Optional] Listen to the summary [podcast episode](https://www.youtube.com/watch?v=F_hJ2Ey4BNc&feature=youtu.be)(18 min) for this unit (created by NotebookLM).
	- Read the [“Prompt Engineering” whitepaper](https://www.kaggle.com/whitepaper-prompt-engineering)(65 pages).
	-  Complete [this code lab](https://www.kaggle.com/code/markishere/day-1-prompting) on Kaggle where you’ll learn prompting fundamentals. Make sure you [phone verify](https://www.kaggle.com/settings) your account before starting, it's necessary for the code labs.

**💡What You’ll Learn**

Today you’ll explore the evolution of LLMs, from transformers to techniques like fine-tuning and inference acceleration. You’ll also get trained in the art of prompt engineering for optimal LLM interaction.

The code lab will walk you through getting started with the Gemini API and cover several prompt techniques and how different parameters impact the prompts.

[![Live Stream](https://i.ytimg.com/vi/kpRyiJUUFxY/hqdefault.jpg)](https://www.youtube.com/watch?v=kpRyiJUUFxY)

<details>
<summary>Summary by gpt-4o</summary>
Overview:
- The course, delivered virtually from November 11–15, focuses on generative AI, covering topics like foundational models, prompt engineering, embeddings, vector databases, AI agents, domain-specific models, and MLOps.
- Participants engage through daily assignments, code labs, white papers, and live Q&A sessions, with discussions facilitated via a Discord channel.
Day 1 Key Highlights:
1. **Foundational Models and Prompt Engineering:**
   - Discussion on training methods for large language models (LLMs) using techniques such as supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF).
   - RLHF uses reward models and human feedback to refine and align models with user preferences.
2. **Technical Advances and Tools:**
   - **Google’s Gemini Models**: Feature 2M token context windows and multimodal capabilities. 
   - **Flash Models**: Focus on high performance and cost efficiency, making generative AI more accessible.
   - **OpenAI API Compatibility**: Simplifies transition between platforms, enabling developers to compare models effortlessly.
3. **Generative AI Applications:**
   - Applications of LLMs in multimodal output, reinforcement learning, and productization.
   - Practical uses include video creation from text documents and enhanced coding workflows.
4. **Evaluation Techniques:**
   - Classical metrics like BLEU and ROUGE for textual outputs.
   - Modern approaches like using LLMs themselves as evaluators (auto-rating).
   - The importance of test-time reasoning and search capabilities for model outputs.
5. **Code Labs and Prompting Techniques:**
   - Demonstrated examples of zero-shot, few-shot, and chain-of-thought prompting.
   - Introduced advanced response customization techniques, such as temperature tuning and structured JSON outputs.
   - Interactive tools like the React Framework for multi-step tasks.
6. **Pop Quiz Recap:**
   - Reinforced core concepts, such as temperature affecting randomness in token prediction, RLHF's role in improving model alignment, and Chain of Thought prompting enhancing reasoning abilities.
</details>

## Day 2 Embeddings and Vector Stores/Databases

**Assignments**

Complete Unit 2: “Embeddings and Vector Stores/Databases”, which is:

-  [Optional] Listen to the summary [podcast episode](https://www.youtube.com/watch?v=1CC39K76Nqs) for this unit (created by NotebookLM).
- Read the [“Embeddings and Vector Stores/Databases” whitepaper](https://www.kaggle.com/whitepaper-embeddings-and-vector-stores)(52 pages).
- Complete these code labs on Kaggle: 
    1. [Build](https://www.kaggle.com/code/markishere/day-2-document-q-a-with-rag) a RAG question-answering system over custom documents
    2. [Explore](https://www.kaggle.com/code/markishere/day-2-embeddings-and-similarity-scores) text similarity with embeddings
    3. [Build](https://www.kaggle.com/code/markishere/day-2-classifying-embeddings-with-keras) a neural classification network with Keras using embeddings

 **💡 What You’ll Learn**

Today you will learn about the conceptual underpinning of embeddings and vector databases and how they can be used to bring live or specialist data into your LLM application. You’ll also explore their geometrical powers for classifying and comparing textual data. 

[![Live Stream](https://i.ytimg.com/vi/86GZC56rQCc/hqdefault.jpg)](https://www.youtube.com/watch?v=86GZC56rQCc)

---

**📋 Reminders**

- Discord is the best place to ask questions – specifically in the [#5dgai-q-and-a](https://discord.com/invite/gNrC9Xut) channel.
- We created a new channel in Discord called [#5dgai-announcements](https://discord.com/invite/RnVCPgX5) that will be used exclusively for course announcements from us.
- We want this course community to be positive and supportive. Please follow Kaggle’s community guidelines found [here](https://www.kaggle.com/community-guidelines).